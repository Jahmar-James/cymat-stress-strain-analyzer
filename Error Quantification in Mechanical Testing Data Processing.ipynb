{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Quantification in Mechanical Testing Data Processing\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In mechanical testing, accurate data analysis is crucial for obtaining reliable results. However, various data processing techniques introduce additional errors that can affect the overall accuracy of the analysis. Understanding and quantifying these errors is essential for ensuring that conclusions drawn from the data are robust and meaningful.\n",
    "\n",
    "This notebook aims to derive and quantify the **theoretical errors** introduced by common data processing techniques used in mechanical testing workflows. While measurement errors are inherent to experimental data, this notebook will focus on the **errors introduced by numerical methods** during data processing. The goal is to track how these errors propagate through processes such as interpolation, unit conversion, regression, integration, and averaging of samples.\n",
    "\n",
    "By systematically accounting for errors introduced at each step, we can better understand how data processing affects the final results, ultimately improving the accuracy and reliability of our analysis.\n",
    "\n",
    "### Scope and Organization\n",
    "\n",
    "This notebook will focus on the following data processing methods, organized by their frequency of use in mechanical testing:\n",
    "\n",
    "1. **Interpolation Errors**:\n",
    "   - Introduced when estimating data points between known values (e.g., force-displacement or stress-strain curves).\n",
    "   - We will derive and quantify errors for linear, polynomial, and spline interpolation methods.\n",
    "\n",
    "2. **Unit Conversion Errors**:\n",
    "   - Introduced when converting between units (e.g., inches to millimeters, pounds to Newtons).\n",
    "   - While Pint ensures exact conversion factors, we will investigate the propagation of errors from the original data through the conversion process.\n",
    "\n",
    "3. **Regression Errors for Modulus Calculation**:\n",
    "   - Introduced when fitting a regression model to calculate mechanical properties such as the modulus of elasticity.\n",
    "   - We will quantify the errors associated with linear regression and determine confidence intervals for the modulus.\n",
    "\n",
    "4. **Integration Errors for Energy Calculation**:\n",
    "   - Introduced when calculating energy from stress-strain curves via numerical integration.\n",
    "   - We will derive the theoretical errors for Simpson’s rule and the trapezoidal rule, and account for error propagation from the input data.\n",
    "\n",
    "5. **Averaging Samples Together**:\n",
    "   - Introduced when averaging data from multiple mechanical tests on different samples.\n",
    "   - We will quantify the error in the mean and account for the propagation of uncertainties from individual datasets.\n",
    "\n",
    "### Goals\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand how to derive the theoretical error for various data processing methods.\n",
    "- Quantify the additional errors introduced at each stage of data processing.\n",
    "- Be able to propagate these errors through different stages of your analysis.\n",
    "- Gain insights into the overall reliability of the final results in your mechanical testing workflow.\n",
    "\n",
    "This notebook will use real mechanical testing data to illustrate each concept and provide practical examples of how to manage and minimize errors during analysis. Let's begin with interpolation errors, one of the most commonly used techniques in mechanical data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Interpolation Errors\n",
    "\n",
    "In mechanical testing workflows, interpolation is often used as an intermediate step for several processes, including:\n",
    "- **Extracting data at specific discrete points** for averaging or comparison.\n",
    "- **Downsampling** data to reduce the dataset size while preserving key information.\n",
    "- **Extrapolating** to obtain derivatives or perform further numerical analysis.\n",
    "\n",
    "Although interpolation provides a useful way to estimate values between known data points, it introduces errors that can propagate through subsequent calculations.\n",
    "\n",
    "#### Sources and Quantification of Interpolation Errors\n",
    "\n",
    "Several factors affect interpolation error, and it is important to understand their impact, especially when interpolation is followed by other operations like averaging or derivative calculations.\n",
    "\n",
    "1. **Data Spacing (h)**: Interpolation errors are generally proportional to the spacing between data points. If data is sampled at regular intervals $ x_i $ with a spacing of $ h $, the error for different interpolation methods can be expressed as:\n",
    "   - **Linear interpolation**: The error is of order $ O(h^2) $. This means that the error decreases quadratically as the data points become closer.\n",
    "   - **Polynomial interpolation (degree n)**: The error is of order $ O(h^{n+1}) $, where $ n $ is the degree of the interpolating polynomial.\n",
    "   - **Spline interpolation (cubic)**: Cubic splines have an error of order $ O(h^4) $, making them more accurate than linear interpolation for smooth functions.\n",
    "\n",
    "2. **Higher-Order Derivatives**: The error in interpolation also depends on the smoothness of the underlying data. For smooth functions $ f(x) $, interpolation error is influenced by the higher-order derivatives of $ f(x) $. For example, in linear interpolation, the error at any point can be approximated by:\n",
    "   $$\n",
    "   E(x) \\approx \\frac{h^2}{8} f''(\\xi),\n",
    "   $$\n",
    "   where $ \\xi $ is some point in the interval $[x_i, x_{i+1}]$, and $ f''(\\xi) $ is the second derivative of the underlying function. Higher-order interpolation methods reduce the impact of these higher derivatives.\n",
    "\n",
    "3. **Error Propagation in Downsampling**: When interpolating to downsample data, the error introduced by interpolation can compound, especially if the interpolated data is used in subsequent averaging or derivative calculations. The cumulative error is often a function of both the interpolation method and the number of data points downsampled.\n",
    "\n",
    "4. **Extrapolation and Derivatives**: Interpolating to obtain derivatives introduces additional challenges. The error in estimating the derivative of a function using interpolation is typically of lower order than the error in the function values themselves. For example:\n",
    "   - Linear interpolation introduces an error of order $ O(h) $ for the first derivative.\n",
    "   - For cubic splines, the error in the first derivative is $ O(h^3) $.\n",
    "\n",
    "#### Error Formulas for Interpolation\n",
    "\n",
    "To summarize the typical error orders for different interpolation methods:\n",
    "- **Linear interpolation**: $ E(x) = O(h^2) $\n",
    "- **Polynomial interpolation** (degree $ n $): $ E(x) = O(h^{n+1}) $\n",
    "- **Cubic spline interpolation**: $ E(x) = O(h^4) $\n",
    "\n",
    "In practical applications, these errors propagate into downstream calculations, so it is essential to account for the total error when interpolating and subsequently performing operations such as averaging or differentiation.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Linear Interpolation (Using `np.polyfit` and `np.poly1d`)\n",
    "This method fits a linear model to your data using `np.polyfit` and generates a linear polynomial (`linear_model`) to estimate values at the points specified in `custom_array`.\n",
    "\n",
    "- **Error Behavior**: As discussed earlier, linear interpolation errors are of order $ O(h^2) $, where $ h $ is the distance between the known data points. The interpolation assumes a straight-line relationship between points, which works well for linear or near-linear data but can introduce significant errors if the underlying relationship is curved.\n",
    "  \n",
    "- **Implication**: If the data has significant curvature between points, this method may underestimate or overestimate values between data points, especially for larger gaps between points.\n",
    "\n",
    "### 2. Cubic Interpolation (Using `CubicSpline`)\n",
    "This method uses `CubicSpline` from SciPy, which fits a piecewise cubic polynomial to your data. It can extrapolate beyond the known data points if `extrapolate=True`.\n",
    "\n",
    "- **Error Behavior**: The error for cubic splines is of order $ O(h^4) $, meaning it decays much faster than linear interpolation for smooth data. However, if the data is noisy or if the function is not smooth, spline interpolation can lead to overfitting and oscillatory behavior (Runge's phenomenon).\n",
    "\n",
    "- **Extrapolation Risk**: Cubic splines tend to behave poorly when extrapolating beyond the range of known data, as the polynomial may behave unpredictably outside of the data range. If extrapolation is not required, it’s often better to limit this.\n",
    "\n",
    "### 3. PCHIP Interpolation (Using `PchipInterpolator`)\n",
    "This method uses the Piecewise Cubic Hermite Interpolating Polynomial (PCHIP) from SciPy, which ensures that the interpolated function is monotonic between data points and does not overshoot.\n",
    "\n",
    "- **Error Behavior**: PCHIP interpolation generally has an error of order $ O(h^2) $ to $ O(h^3) $, depending on the smoothness of the data. It is designed to handle data with sharp transitions or steep gradients, preventing the overshooting and oscillation seen in cubic splines. While it doesn’t provide the smoothness of cubic splines, it is more stable and reliable in cases where the data changes rapidly.\n",
    "\n",
    "- **Use Case**: PCHIP is particularly useful for preserving the shape of the data, making it less prone to introducing artifacts, especially in the presence of sharp changes or irregular data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpolation Methods and Error Growth\n",
    "\n",
    "The interpolation methods used in this workflow are selected based on the data characteristics and the need for extrapolation or smoothness. Below are the key interpolation methods applied:\n",
    "\n",
    "1. **Linear Interpolation (`np.polyfit`)**: Fits a straight line between data points. The interpolation error is proportional to the square of the distance between points ($ O(h^2) $). Suitable for near-linear data or cases where preserving simplicity is more important than capturing curvature.\n",
    "\n",
    "2. **Cubic Interpolation (`CubicSpline`)**: Uses piecewise cubic polynomials to fit the data. The error scales as $ O(h^4) $, meaning it is more accurate for smooth functions but may overfit in the presence of noise or irregular data. Extrapolation using cubic splines is prone to introducing large errors due to oscillations outside the data range.\n",
    "\n",
    "3. **PCHIP Interpolation (`PchipInterpolator`)**: Ensures that the interpolated function is monotonic, preventing overshooting and oscillations. It is ideal for data with sharp transitions or steep gradients. The error ranges from $ O(h^2) $ to $ O(h^3) $, depending on the data's smoothness.\n",
    "\n",
    "4. **`interp1d` (SciPy)**: Allows flexible interpolation (linear, cubic, etc.). The error depends on the selected method:\n",
    "   - Linear: $ O(h^2) $\n",
    "   - Cubic: $ O(h^4) $\n",
    "   - Extrapolation can introduce significant errors, particularly for higher-order methods.\n",
    "\n",
    "Understanding the error introduced by each method helps in selecting the right approach for the specific task, especially when averaging, calculating derivatives, or downsampling data.\n",
    "\n",
    "---\n",
    "\n",
    "### General Approach to Estimating Interpolation Error\n",
    "\n",
    "1. **Obtain Spacing $ h $**: First, calculate the average (or maximum) spacing between the data points. This is crucial because the interpolation error depends directly on the spacing between the known data points.\n",
    "\n",
    "2. **Estimate the Derivative**: For interpolation methods like linear or cubic, the error formula involves higher-order derivatives (e.g., $ f''(x) $ for linear, $ f''''(x) $ for cubic). These derivatives can be estimated numerically from the data.\n",
    "\n",
    "3. **Apply the Error Formula**: Use the appropriate error formula based on the method:\n",
    "   - **Linear interpolation**: $ E(x) \\approx \\frac{h^2}{8} \\max |f''(x)| $\n",
    "   - **Cubic spline interpolation**: $ E(x) \\approx \\frac{h^4}{384} \\max |f''''(x)| $\n",
    "   - **PCHIP interpolation**: We can use a similar approach to cubic but estimate a derivative of lower order due to its smoother properties. $ E(x) \\approx \\frac{h^3}{24} \\max |f''(x)| $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def estimate_interpolation_error(df, interp_column, target_column, interpolation_method):\n",
    "    df_copy = df.copy()\n",
    "    # Step 1: Calculate average spacing h between points\n",
    "    h = np.mean(np.diff(df[interp_column]))\n",
    "\n",
    "    # Step 2: Estimate derivatives based on the interpolation method\n",
    "    if interpolation_method == \"linear\":\n",
    "        # Estimate second derivative numerically for linear interpolation\n",
    "        second_derivative = np.gradient(np.gradient(df[target_column], df[interp_column]), df[interp_column])\n",
    "        max_second_derivative = np.max(np.abs(second_derivative))\n",
    "        # Step 3: Apply the error formula for linear interpolation\n",
    "        error_estimate = (h**2 / 8) * max_second_derivative\n",
    "    \n",
    "    elif interpolation_method == \"cubic\":\n",
    "        # Estimate fourth derivative numerically for cubic interpolation\n",
    "        second_derivative = np.gradient(np.gradient(df[target_column], df[interp_column]), df[interp_column])\n",
    "        fourth_derivative = np.gradient(np.gradient(second_derivative, df[interp_column]), df[interp_column])\n",
    "        max_fourth_derivative = np.max(np.abs(fourth_derivative))\n",
    "        # Step 3: Apply the error formula for cubic interpolation\n",
    "        error_estimate = (h**4 / 384) * max_fourth_derivative\n",
    "    \n",
    "    elif interpolation_method == \"pchip\":\n",
    "        # PCHIP is monotonic, and we can use a second derivative as a proxy for error estimation.\n",
    "        second_derivative = np.gradient(np.gradient(df[target_column], df[interp_column]), df[interp_column])\n",
    "        max_second_derivative = np.max(np.abs(second_derivative))\n",
    "        # Conservative error estimate for PCHIP using second derivative (similar to cubic)\n",
    "        error_estimate = (h**3 / 24) * max_second_derivative\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported interpolation method: {interpolation_method}\")\n",
    "    \n",
    "    return error_estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear interpolation error estimate: 2.8447542874744303e-16 (analytical: 0.0) which is 2.8447542874744303e-16 off\n",
      "Quadratic interpolation error estimate: 0.06925207756232794 (analytical: 0.06925207756232683) which is 1.1102230246251565e-15 off\n",
      "Sine interpolation error estimate: 0.00016583796788432293 (analytical: 0.00019982709361243881) which is 3.398912572811588e-05 off\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the test functions\n",
    "def linear_function(x):\n",
    "    return 2 * x + 1\n",
    "\n",
    "def quadratic_function(x):\n",
    "    return x**2\n",
    "\n",
    "def sine_function(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "# Generate some test data for each function\n",
    "x_values = np.linspace(0, 10, 20)  # 20 evenly spaced points between 0 and 10\n",
    "h_step = np.mean(np.diff(x_values))  # Average spacing between points\n",
    "\n",
    "# Create DataFrames for each test case\n",
    "df_linear = pd.DataFrame({'x': x_values, 'y': linear_function(x_values)})\n",
    "df_quadratic = pd.DataFrame({'x': x_values, 'y': quadratic_function(x_values)})\n",
    "df_sine = pd.DataFrame({'x': x_values, 'y': sine_function(x_values)})\n",
    "\n",
    "# Function to test the interpolation error estimation\n",
    "def test_interpolation_error(df, interp_column, target_column, interpolation_method):\n",
    "    error_estimate = estimate_interpolation_error(df, interp_column, target_column, interpolation_method)\n",
    "    return error_estimate\n",
    "\n",
    "# Analytical error for quadratic function (linear interpolation)\n",
    "# E(x) = (h^2 / 8) * max(f'')\n",
    "\n",
    "max_second_derivative_linear = 0  # f''(x) for 2x + 1 is 0\n",
    "max_second_derivative_quadratic = 2  # f''(x) for x^2 is 2\n",
    "\n",
    "# Test the linear interpolation error estimation on the linear function\n",
    "linear_error_estimate = test_interpolation_error(df_linear, 'x', 'y', 'linear')\n",
    "linear_analytical_error = (h_step**2 / 8) * max_second_derivative_linear\n",
    "linear_error_discrepancy = np.abs(linear_analytical_error - linear_error_estimate)\n",
    "\n",
    "quadratic_analytical_error = (h_step**2 / 8) * max_second_derivative_quadratic\n",
    "# Test the linear interpolation error estimation on the quadratic function\n",
    "quadratic_linear_error_esitmate = test_interpolation_error(df_quadratic, 'x', 'y', 'linear')\n",
    "quadratic_linear_error_discrepancy = np.abs(quadratic_analytical_error - quadratic_linear_error_esitmate)\n",
    "\n",
    "# Analytical error for sine function (cubic interpolation)\n",
    "# E(x) = (h^4 / 384) * max(f'''')\n",
    "max_fourth_derivative_sine = 1  # f''''(x) for sin(x) is sin(x), with max of 1\n",
    "sine_analytical_error = (h_step**4 / 384) * max_fourth_derivative_sine\n",
    "# Test the cubic interpolation error estimation on the sine function\n",
    "sine_cubic_error_esitmate = test_interpolation_error(df_sine, 'x', 'y', 'cubic')\n",
    "sine_cubic_error_discrepancy = np.abs(sine_analytical_error - sine_cubic_error_esitmate)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Linear interpolation error estimate: {linear_error_estimate} (analytical: {linear_analytical_error}) which is {linear_error_discrepancy} off\")\n",
    "print(f\"Quadratic interpolation error estimate: {quadratic_linear_error_esitmate} (analytical: {quadratic_analytical_error}) which is {quadratic_linear_error_discrepancy} off\")\n",
    "print(f\"Sine interpolation error estimate: {sine_cubic_error_esitmate} (analytical: {sine_analytical_error}) which is {sine_cubic_error_discrepancy} off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Interpolation Error Summary\n",
    "\n",
    "We tested the interpolation error estimation function on three functions with known analytical solutions: a linear function, a quadratic function, and a sine function. The results are summarized below:\n",
    "\n",
    "1. **Linear Function** $ f(x) = 2x + 1 $ with Linear Interpolation:\n",
    "   - **Estimated Error**: $ 2.84 \\times 10^{-16} $\n",
    "   - **Analytical Error**: $ 0 $\n",
    "   - **Discrepancy**: $ 2.84 \\times 10^{-16} $\n",
    "\n",
    "   The error for the linear function is effectively zero, confirming that linear interpolation introduces negligible error for a perfectly linear function.\n",
    "\n",
    "2. **Quadratic Function** $ f(x) = x^2 $ with Linear Interpolation:\n",
    "   - **Estimated Error**: $ 0.069252 $\n",
    "   - **Analytical Error**: $ 0.069252 $\n",
    "   - **Discrepancy**: $ 1.11 \\times 10^{-15} $\n",
    "\n",
    "   The error estimate for linear interpolation is highly accurate for quadratic data. However, higher-order methods, such as cubic interpolation, could reduce the error further.\n",
    "\n",
    "3. **Sine Function** $ f(x) = \\sin(x) $ with Cubic Interpolation:\n",
    "   - **Estimated Error**: $ 0.0001658 $\n",
    "   - **Analytical Error**: $ 0.0001998 $\n",
    "   - **Discrepancy**: $ 3.4 \\times 10^{-5} $\n",
    "\n",
    "   Cubic interpolation provides a close approximation to the analytical error, demonstrating its effectiveness for smooth functions like sine waves. Minor discrepancies can arise from the numerical estimation of the fourth derivative.\n",
    "\n",
    "### Impact on Uncertainty\n",
    "\n",
    "In real-world scenarios, where the true function is unknown, these error estimates provide an important diagnostic tool:\n",
    "\n",
    "1. **Identifying Issues**: If the estimated interpolation error is large, it can signal that the chosen interpolation method does not fit the data well. For instance, if you apply linear interpolation to a dataset with significant curvature, the error estimate will likely be high.\n",
    "\n",
    "2. **Comparing Interpolation Methods**: By testing multiple interpolation methods (e.g., linear, cubic, PCHIP) and comparing the estimated errors, you can identify which technique introduces the least uncertainty. A smaller error suggests that the method fits the data more closely, even if the true function is unknown.\n",
    "\n",
    "3. **Actionable Insights**: If the error is consistently high, it might indicate that the data requires a higher-order interpolation method (such as cubic or spline interpolation). Conversely, if the error is small, the simpler method might suffice, minimizing computational complexity.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Set error thresholds**: Establish a threshold for acceptable error. If the error exceeds this threshold, consider switching to a more sophisticated interpolation technique.\n",
    "  \n",
    "- **Refine data processing**: Based on the interpolation error estimates, continuously refine the choice of interpolation techniques in your workflow. This will help improve the accuracy of your processed results, especially when performing operations like averaging or differentiation on interpolated data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Unit Conversion and Its Impact\n",
    "\n",
    "In this workflow, unit conversion plays a minimal role in affecting the accuracy of the results due to the following reasons:\n",
    "\n",
    "1. **Internal Standard Units**: All data is processed and stored in a consistent internal unit system (e.g., SI units). This eliminates the possibility of errors due to mixed units during calculations.\n",
    "\n",
    "2. **Localized Conversions**: Unit conversions only occur at two points:\n",
    "   - When user inputs data in non-standard units, it is converted to the internal unit system for processing.\n",
    "   - When data is converted to user-requested units for plotting or visualization, which does not impact the processed data or final results.\n",
    "\n",
    "3. **Precision with `Pint`**: The `Pint` library is used for all unit conversions, ensuring that exact conversion factors are applied, thereby minimizing any risk of rounding or truncation errors.\n",
    "\n",
    "### Negligible Impact on Uncertainty\n",
    "\n",
    "Since conversions are isolated to input and output stages, and calculations are performed in standardized units, the errors introduced by unit conversions are negligible. Additionally, any rounding errors are localized and unlikely to affect the overall accuracy of the workflow. Therefore, unit conversion has no meaningful impact on the uncertainty of the final results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regression Errors for Modulus Calculation\n",
    "\n",
    "In mechanical testing, regression analysis is commonly used to calculate material properties such as the modulus of elasticity. The most common method is linear regression, which fits a straight line to the stress-strain curve to determine the slope (modulus).\n",
    "\n",
    "Great! Let's dive into the **regression error** section, particularly focusing on **linear regression** for calculating mechanical properties such as the modulus. In this case, since you’re always performing **linear fitting** to determine the modulus (e.g., elastic modulus from stress-strain data), we'll focus on the common sources of error, especially those related to data selection and fitting.\n",
    "\n",
    "### Key Sources of Regression Error\n",
    "\n",
    "1. **Data Selection Error (Fitting Range)**:\n",
    "   - The range of data points selected for fitting can have a significant impact on the accuracy of the linear regression.\n",
    "   - If the chosen range does not represent the linear portion of the data (e.g., the elastic region in stress-strain curves), the modulus calculation can be inaccurate.\n",
    "   - **Error Impact**: Poor range selection introduces systematic error, leading to incorrect results.\n",
    "\n",
    "2. **Measurement Noise**:\n",
    "   - Real-world data often contains noise from measurement instruments, sample preparation, or testing conditions. Noise introduces uncertainty into the regression model by making it harder to find the true linear relationship.\n",
    "   - **Error Impact**: Noise leads to a less accurate slope (modulus) estimate and increases the uncertainty in the regression coefficients.\n",
    "\n",
    "3. **Outliers and Inconsistent Data**:\n",
    "   - Outliers or inconsistent data points can skew the results of the linear fit, especially if they are not properly handled.\n",
    "   - **Error Impact**: Outliers can disproportionately influence the slope, leading to an inaccurate modulus.\n",
    "\n",
    "### Quantifying Regression Error: Metrics for Linear Fitting\n",
    "\n",
    "For linear regression, several key metrics can be used to quantify the error and uncertainty in the fit:\n",
    "\n",
    "1. **Standard Error of the Slope**:\n",
    "     **Purpose**: The SE quantifies the **uncertainty** in the slope (modulus) estimate. It tells you how much the calculated slope may vary due to random errors in the data.\n",
    "   - **Formula**: \n",
    "     $$\n",
    "     \\text{SE} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2} \\times \\frac{1}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}}\n",
    "     $$\n",
    "   - **Meaning**: A smaller SE indicates a more reliable estimate of the modulus, while a larger SE suggests higher uncertainty due to noise or poor data quality.\n",
    "   - **Impact**: SE represents the uncertainty in the modulus. You can use this value to communicate how confident you are in the calculated modulus. A large SE suggests the results may be unreliable, and you may need to refine the range or data quality.\n",
    "\n",
    "2. **Coefficient of Determination ($R^2$)**:\n",
    "   - **Purpose**: The $ R^2 $ value measures the **goodness-of-fit** of the linear regression model. It ranges from 0 to 1, with values closer to 1 indicating a better fit.\n",
    "   - **Formula**: \n",
    "     $$\n",
    "     R^2 = 1 - \\frac{\\sum (y_i - \\hat{y_i})^2}{\\sum (y_i - \\bar{y})^2}\n",
    "     $$\n",
    "    - **Meaning**: A high $ R^2 $ value means the data fits well to a line, indicating that the selected range is appropriate for calculating the modulus. A low $ R^2 $ suggests that the data may not be linear or that the chosen range is poor.\n",
    "   - **Impact**: If $ R^2 $ is too low, it might indicate that the user should choose a new range. While $ R^2 $ doesn’t directly quantify uncertainty in the modulus, it helps assess whether the data is appropriate for linear regression. - **Determine a threshold for acceptable $ R^2 $**\n",
    "\n",
    "3. **Confidence Intervals**:\n",
    "   - **Purpose**: Confidence intervals can be calculated for the slope to provide a range within which the true modulus is likely to lie. A narrower confidence interval indicates higher confidence in the estimate.\n",
    "   - **Formula** for 95% confidence interval of slope:\n",
    "     $$\n",
    "     \\hat{\\beta} \\pm t_{\\alpha/2, n-2} \\cdot \\text{SE}\n",
    "     $$\n",
    "   - $t_{\\alpha/2, n-2}$ is the t-distribution critical value for the desired confidence level and sample size.\n",
    "   - **Impact**: Confidence intervals complement SE by providing an explicit range for the slope estimate, though SE alone is often sufficient to represent uncertainty in the modulus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Example function to perform linear regression and quantify error\n",
    "def estimate_regression_error(x, y):\n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "    \n",
    "    # Calculate R^2\n",
    "    r_squared = r_value**2\n",
    "\n",
    "    # Return regression metrics\n",
    "    return {\n",
    "        'slope': slope,\n",
    "        'intercept': intercept,\n",
    "        'R_squared': r_squared,\n",
    "        'standard_error': std_err\n",
    "    }\n",
    "\n",
    "# Example usage with test data\n",
    "x_data = np.linspace(0, 10, 50)  # Example x-values\n",
    "y_data = 3 * x_data + np.random.normal(0, 0.5, size=x_data.shape)  # Linear y-values with noise\n",
    "\n",
    "regression_results = estimate_regression_error(x_data, y_data)\n",
    "\n",
    "# Print results\n",
    "print(f\"Slope (modulus): {regression_results['slope']}\")\n",
    "print(f\"Intercept: {regression_results['intercept']}\")\n",
    "print(f\"R-squared: {regression_results['R_squared']}\")\n",
    "print(f\"Standard Error of Slope: {regression_results['standard_error']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
